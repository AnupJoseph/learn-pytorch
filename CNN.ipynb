{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"d9b4ccf0-2caa-4e31-b5ee-01e38e3ed844"},"source":"import torch\r\nimport torch.nn as nn\r\nimport torchvision.transforms as transforms\r\nimport torchvision.datasets as datasets","execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_dataset = datasets.MNIST(root='./data', \r\n                            train=True, \r\n                            transform=transforms.ToTensor(),\r\n                            download=True)\r\n\r\ntest_dataset = datasets.MNIST(root='./data', \r\n                           train=False, \r\n                           transform=transforms.ToTensor())","metadata":{"tags":[],"cell_id":"7decd4ef-0cb7-4442-92b0-b98e2ee72502"},"outputs":[],"execution_count":4},{"cell_type":"code","source":"batch_size = 100\r\nn_iters = 3000\r\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\r\nnum_epochs = int(num_epochs)\r\n\r\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\r\n\r\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"tags":[],"cell_id":"428e7a01-9aca-4b78-b30e-dd6a7545d638"},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class CNNmodel(nn.Module):\r\n    def __init__(self):\r\n        super(CNNmodel,self).__init__()\r\n\r\n        self.cnn1 = nn.Conv2d(in_channels=1,out_channels=16,kernel_size=5,stride=1,padding=2)\r\n        self.relu1 = nn.ReLU()\r\n\r\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\r\n\r\n        self.cnn2 = nn.Conv2d(in_channels=16,out_channels=32,kernel_size=5,stride=1,padding=2)\r\n        self.relu2 = nn.ReLU()\r\n\r\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\r\n\r\n        self.fc1 = nn.Linear(32*7*7,10)\r\n\r\n    def forward(self,x):\r\n\r\n        out = self.cnn1(x)\r\n        out = self.relu1(out)\r\n\r\n        out = self.maxpool1(out)\r\n\r\n        out = self.cnn2(out)\r\n        out = self.relu2(out)\r\n\r\n        out = self.maxpool2(out)\r\n\r\n        out = out.view(out.size(0), -1)\r\n\r\n        out = self.fc1(out)\r\n        return out\r\n\r\n\r\n\r\n","metadata":{"tags":[],"cell_id":"b0d90bd0-2b1b-4629-b3d1-5d62dcb385b7"},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model = CNNmodel()\r\ncriterion = nn.CrossEntropyLoss()\r\nlearning_rate = 0.01\r\n\r\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  ","metadata":{"tags":[],"cell_id":"0ce07f69-6a1f-49ee-b1b5-09b965be6b73"},"outputs":[],"execution_count":7},{"cell_type":"code","source":"iter = 0\r\nfor epoch in range(num_epochs):\r\n    for i, (images, labels) in enumerate(train_loader):\r\n        # Load images\r\n        images = images.requires_grad_()\r\n\r\n        # Clear gradients w.r.t. parameters\r\n        optimizer.zero_grad()\r\n\r\n        # Forward pass to get output/logits\r\n        outputs = model(images)\r\n\r\n        # Calculate Loss: softmax --> cross entropy loss\r\n        loss = criterion(outputs, labels)\r\n\r\n        # Getting gradients w.r.t. parameters\r\n        loss.backward()\r\n\r\n        # Updating parameters\r\n        optimizer.step()\r\n\r\n        iter += 1\r\n\r\n        if iter % 500 == 0:\r\n            # Calculate Accuracy         \r\n            correct = 0\r\n            total = 0\r\n            # Iterate through test dataset\r\n            for images, labels in test_loader:\r\n                # Load images\r\n                images = images.requires_grad_()\r\n\r\n                # Forward pass only to get logits/output\r\n                outputs = model(images)\r\n\r\n                # Get predictions from the maximum value\r\n                _, predicted = torch.max(outputs.data, 1)\r\n\r\n                # Total number of labels\r\n                total += labels.size(0)\r\n\r\n                # Total correct predictions\r\n                correct += (predicted == labels).sum()\r\n\r\n            accuracy = 100 * correct / total\r\n\r\n            # Print Loss\r\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))","metadata":{"tags":[],"cell_id":"91358ed9-0543-43ba-a197-df3caabb5c4e"},"outputs":[{"name":"stderr","text":"/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\nIteration: 500. Loss: 0.354157418012619. Accuracy: 89\nIteration: 1000. Loss: 0.27558544278144836. Accuracy: 92\nIteration: 1500. Loss: 0.23500339686870575. Accuracy: 94\nIteration: 2000. Loss: 0.1748580038547516. Accuracy: 95\nIteration: 2500. Loss: 0.20146234333515167. Accuracy: 95\nIteration: 3000. Loss: 0.12026937305927277. Accuracy: 96\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\r\nimport torch.nn as nn\r\nimport torchvision.transforms as transforms\r\nimport torchvision.datasets as dsets\r\n\r\n'''\r\nSTEP 1: LOADING DATASET\r\n'''\r\n\r\ntrain_dataset = dsets.MNIST(root='./data', \r\n                            train=True, \r\n                            transform=transforms.ToTensor(),\r\n                            download=True)\r\n\r\ntest_dataset = dsets.MNIST(root='./data', \r\n                           train=False, \r\n                           transform=transforms.ToTensor())\r\n\r\n'''\r\nSTEP 2: MAKING DATASET ITERABLE\r\n'''\r\n\r\nbatch_size = 100\r\nn_iters = 3000\r\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\r\nnum_epochs = int(num_epochs)\r\n\r\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \r\n                                           batch_size=batch_size, \r\n                                           shuffle=True)\r\n\r\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \r\n                                          batch_size=batch_size, \r\n                                          shuffle=False)\r\n\r\n'''\r\nSTEP 3: CREATE MODEL CLASS\r\n'''\r\nclass CNNModel(nn.Module):\r\n    def __init__(self):\r\n        super(CNNModel, self).__init__()\r\n\r\n        # Convolution 1\r\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\r\n        self.relu1 = nn.ReLU()\r\n\r\n        # Average pool 1\r\n        self.avgpool1 = nn.AvgPool2d(kernel_size=2)\r\n\r\n        # Convolution 2\r\n        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\r\n        self.relu2 = nn.ReLU()\r\n\r\n        # Average pool 2\r\n        self.avgpool2 = nn.AvgPool2d(kernel_size=2)\r\n\r\n        # Fully connected 1 (readout)\r\n        self.fc1 = nn.Linear(32 * 7 * 7, 10) \r\n\r\n    def forward(self, x):\r\n        # Convolution 1\r\n        out = self.cnn1(x)\r\n        out = self.relu1(out)\r\n\r\n        # Average pool 1\r\n        out = self.avgpool1(out)\r\n\r\n        # Convolution 2 \r\n        out = self.cnn2(out)\r\n        out = self.relu2(out)\r\n\r\n        # Max pool 2 \r\n        out = self.avgpool2(out)\r\n\r\n        # Resize\r\n        # Original size: (100, 32, 7, 7)\r\n        # out.size(0): 100\r\n        # New out size: (100, 32*7*7)\r\n        out = out.view(out.size(0), -1)\r\n\r\n        # Linear function (readout)\r\n        out = self.fc1(out)\r\n\r\n        return out\r\n\r\n'''\r\nSTEP 4: INSTANTIATE MODEL CLASS\r\n'''\r\n\r\nmodel = CNNModel()\r\n\r\n'''\r\nSTEP 5: INSTANTIATE LOSS CLASS\r\n'''\r\ncriterion = nn.CrossEntropyLoss()\r\n\r\n\r\n'''\r\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\r\n'''\r\nlearning_rate = 0.01\r\n\r\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\r\n\r\n'''\r\nSTEP 7: TRAIN THE MODEL\r\n'''\r\niter = 0\r\nfor epoch in range(num_epochs):\r\n    for i, (images, labels) in enumerate(train_loader):\r\n        # Load images as tensors with gradient accumulation abilities\r\n        images = images.requires_grad_()\r\n\r\n        # Clear gradients w.r.t. parameters\r\n        optimizer.zero_grad()\r\n\r\n        # Forward pass to get output/logits\r\n        outputs = model(images)\r\n\r\n        # Calculate Loss: softmax --> cross entropy loss\r\n        loss = criterion(outputs, labels)\r\n\r\n        # Getting gradients w.r.t. parameters\r\n        loss.backward()\r\n\r\n        # Updating parameters\r\n        optimizer.step()\r\n\r\n        iter += 1\r\n\r\n        if iter % 500 == 0:\r\n            # Calculate Accuracy         \r\n            correct = 0\r\n            total = 0\r\n            # Iterate through test dataset\r\n            for images, labels in test_loader:\r\n                # Load images to tensors with gradient accumulation abilities\r\n                images = images.requires_grad_()\r\n\r\n                # Forward pass only to get logits/output\r\n                outputs = model(images)\r\n\r\n                # Get predictions from the maximum value\r\n                _, predicted = torch.max(outputs.data, 1)\r\n\r\n                # Total number of labels\r\n                total += labels.size(0)\r\n\r\n                # Total correct predictions\r\n                correct += (predicted == labels).sum()\r\n\r\n            accuracy = 100 * correct / total\r\n\r\n            # Print Loss\r\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))","metadata":{"tags":[],"cell_id":"666fd09e-f98a-4eb2-a0cd-8d2684303d3c"},"outputs":[{"name":"stdout","text":"Iteration: 500. Loss: 0.4947304427623749. Accuracy: 83\nIteration: 1000. Loss: 0.2464093714952469. Accuracy: 89\nIteration: 1500. Loss: 0.3538453280925751. Accuracy: 90\nIteration: 2000. Loss: 0.1877150982618332. Accuracy: 91\nIteration: 2500. Loss: 0.2555699646472931. Accuracy: 92\nIteration: 3000. Loss: 0.1887694001197815. Accuracy: 93\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\r\nimport torch.nn as nn\r\nimport torchvision.transforms as transforms\r\nimport torchvision.datasets as dsets\r\n\r\n'''\r\nSTEP 1: LOADING DATASET\r\n'''\r\n\r\ntrain_dataset = dsets.MNIST(root='./data', \r\n                            train=True, \r\n                            transform=transforms.ToTensor(),\r\n                            download=True)\r\n\r\ntest_dataset = dsets.MNIST(root='./data', \r\n                           train=False, \r\n                           transform=transforms.ToTensor())\r\n\r\n'''\r\nSTEP 2: MAKING DATASET ITERABLE\r\n'''\r\n\r\nbatch_size = 100\r\nn_iters = 3000\r\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\r\nnum_epochs = int(num_epochs)\r\n\r\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \r\n                                           batch_size=batch_size, \r\n                                           shuffle=True)\r\n\r\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \r\n                                          batch_size=batch_size, \r\n                                          shuffle=False)\r\n\r\n'''\r\nSTEP 3: CREATE MODEL CLASS\r\n'''\r\nclass CNNModel(nn.Module):\r\n    def __init__(self):\r\n        super(CNNModel, self).__init__()\r\n\r\n        # Convolution 1\r\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\r\n        self.relu1 = nn.ReLU()\r\n\r\n        # Max pool 1\r\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\r\n\r\n        # Convolution 2\r\n        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\r\n        self.relu2 = nn.ReLU()\r\n\r\n        # Max pool 2\r\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\r\n\r\n        # Fully connected 1 (readout)\r\n        self.fc1 = nn.Linear(32 * 4 * 4, 10) \r\n\r\n    def forward(self, x):\r\n        # Convolution 1\r\n        out = self.cnn1(x)\r\n        out = self.relu1(out)\r\n\r\n        # Max pool 1\r\n        out = self.maxpool1(out)\r\n\r\n        # Convolution 2 \r\n        out = self.cnn2(out)\r\n        out = self.relu2(out)\r\n\r\n        # Max pool 2 \r\n        out = self.maxpool2(out)\r\n\r\n        # Resize\r\n        # Original size: (100, 32, 7, 7)\r\n        # out.size(0): 100\r\n        # New out size: (100, 32*7*7)\r\n        out = out.view(out.size(0), -1)\r\n\r\n        # Linear function (readout)\r\n        out = self.fc1(out)\r\n\r\n        return out\r\n\r\n'''\r\nSTEP 4: INSTANTIATE MODEL CLASS\r\n'''\r\n\r\nmodel = CNNModel()\r\n\r\n'''\r\nSTEP 5: INSTANTIATE LOSS CLASS\r\n'''\r\ncriterion = nn.CrossEntropyLoss()\r\n\r\n\r\n'''\r\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\r\n'''\r\nlearning_rate = 0.01\r\n\r\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\r\n\r\n'''\r\nSTEP 7: TRAIN THE MODEL\r\n'''\r\niter = 0\r\nfor epoch in range(num_epochs):\r\n    for i, (images, labels) in enumerate(train_loader):\r\n        # Load images as tensors with gradient accumulation abilities\r\n        images = images.requires_grad_()\r\n\r\n        # Clear gradients w.r.t. parameters\r\n        optimizer.zero_grad()\r\n\r\n        # Forward pass to get output/logits\r\n        outputs = model(images)\r\n\r\n        # Calculate Loss: softmax --> cross entropy loss\r\n        loss = criterion(outputs, labels)\r\n\r\n        # Getting gradients w.r.t. parameters\r\n        loss.backward()\r\n\r\n        # Updating parameters\r\n        optimizer.step()\r\n\r\n        iter += 1\r\n\r\n        if iter % 500 == 0:\r\n            # Calculate Accuracy         \r\n            correct = 0\r\n            total = 0\r\n            # Iterate through test dataset\r\n            for images, labels in test_loader:\r\n                # Load images to tensors with gradient accumulation abilities\r\n                images = images.requires_grad_()\r\n\r\n                # Forward pass only to get logits/output\r\n                outputs = model(images)\r\n\r\n                # Get predictions from the maximum value\r\n                _, predicted = torch.max(outputs.data, 1)\r\n\r\n                # Total number of labels\r\n                total += labels.size(0)\r\n\r\n                # Total correct predictions\r\n                correct += (predicted == labels).sum()\r\n\r\n            accuracy = 100 * correct / total\r\n\r\n            # Print Loss\r\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))","metadata":{"tags":[],"cell_id":"263eb315-0d11-47bd-a010-75ba4226c1ab"},"outputs":[{"name":"stdout","text":"Iteration: 500. Loss: 0.22677606344223022. Accuracy: 90\nIteration: 1000. Loss: 0.2722513675689697. Accuracy: 93\nIteration: 1500. Loss: 0.11430569738149643. Accuracy: 94\nIteration: 2000. Loss: 0.1074848547577858. Accuracy: 95\nIteration: 2500. Loss: 0.2023341953754425. Accuracy: 96\nIteration: 3000. Loss: 0.04929247871041298. Accuracy: 96\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import torch\r\nimport torch.nn as nn\r\nimport torchvision.transforms as transforms\r\nimport torchvision.datasets as dsets \r\n\r\n'''\r\nSTEP 1: LOADING DATASET\r\n'''\r\n\r\ntrain_dataset = dsets.MNIST(root='./data', \r\n                            train=True, \r\n                            transform=transforms.ToTensor(),\r\n                            download=True)\r\n\r\ntest_dataset = dsets.MNIST(root='./data', \r\n                           train=False, \r\n                           transform=transforms.ToTensor())\r\n\r\n'''\r\nSTEP 2: MAKING DATASET ITERABLE\r\n'''\r\n\r\nbatch_size = 100\r\nn_iters = 3000\r\nnum_epochs = n_iters / (len(train_dataset) / batch_size)\r\nnum_epochs = int(num_epochs)\r\n\r\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \r\n                                           batch_size=batch_size, \r\n                                           shuffle=True)\r\n\r\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \r\n                                          batch_size=batch_size, \r\n                                          shuffle=False)\r\n\r\n'''\r\nSTEP 3: CREATE MODEL CLASS\r\n'''\r\nclass CNNModel(nn.Module):\r\n    def __init__(self):\r\n        super(CNNModel, self).__init__()\r\n\r\n        # Convolution 1\r\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\r\n        self.relu1 = nn.ReLU()\r\n\r\n        # Max pool 1\r\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\r\n\r\n        # Convolution 2\r\n        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\r\n        self.relu2 = nn.ReLU()\r\n\r\n        # Max pool 2\r\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\r\n\r\n        # Fully connected 1 (readout)\r\n        self.fc1 = nn.Linear(32 * 4 * 4, 10) \r\n\r\n    def forward(self, x):\r\n        # Convolution 1\r\n        out = self.cnn1(x)\r\n        out = self.relu1(out)\r\n\r\n        # Max pool 1\r\n        out = self.maxpool1(out)\r\n\r\n        # Convolution 2 \r\n        out = self.cnn2(out)\r\n        out = self.relu2(out)\r\n\r\n        # Max pool 2 \r\n        out = self.maxpool2(out)\r\n\r\n        # Resize\r\n        # Original size: (100, 32, 7, 7)\r\n        # out.size(0): 100\r\n        # New out size: (100, 32*7*7)\r\n        out = out.view(out.size(0), -1)\r\n\r\n        # Linear function (readout)\r\n        out = self.fc1(out)\r\n\r\n        return out\r\n\r\n'''\r\nSTEP 4: INSTANTIATE MODEL CLASS\r\n'''\r\n\r\nmodel = CNNModel()\r\n\r\n#######################\r\n#  USE GPU FOR MODEL  #\r\n#######################\r\n\r\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\nmodel.to(device)\r\n\r\n'''\r\nSTEP 5: INSTANTIATE LOSS CLASS\r\n'''\r\ncriterion = nn.CrossEntropyLoss()\r\n\r\n\r\n'''\r\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\r\n'''\r\nlearning_rate = 0.01\r\n\r\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\r\n\r\n'''\r\nSTEP 7: TRAIN THE MODEL\r\n'''\r\niter = 0\r\nfor epoch in range(num_epochs):\r\n    for i, (images, labels) in enumerate(train_loader):\r\n\r\n        #######################\r\n        #  USE GPU FOR MODEL  #\r\n        #######################\r\n        images = images.requires_grad_().to(device)\r\n        labels = labels.to(device)\r\n\r\n        # Clear gradients w.r.t. parameters\r\n        optimizer.zero_grad()\r\n\r\n        # Forward pass to get output/logits\r\n        outputs = model(images)\r\n\r\n        # Calculate Loss: softmax --> cross entropy loss\r\n        loss = criterion(outputs, labels)\r\n\r\n        # Getting gradients w.r.t. parameters\r\n        loss.backward()\r\n\r\n        # Updating parameters\r\n        optimizer.step()\r\n\r\n        iter += 1\r\n\r\n        if iter % 500 == 0:\r\n            # Calculate Accuracy         \r\n            correct = 0\r\n            total = 0\r\n            # Iterate through test dataset\r\n            for images, labels in test_loader:\r\n                #######################\r\n                #  USE GPU FOR MODEL  #\r\n                #######################\r\n                images = images.requires_grad_().to(device)\r\n                labels = labels.to(device)\r\n\r\n                # Forward pass only to get logits/output\r\n                outputs = model(images)\r\n\r\n                # Get predictions from the maximum value\r\n                _, predicted = torch.max(outputs.data, 1)\r\n\r\n                # Total number of labels\r\n                total += labels.size(0)\r\n\r\n                #######################\r\n                #  USE GPU FOR MODEL  #\r\n                #######################\r\n                # Total correct predictions\r\n                if torch.cuda.is_available():\r\n                    correct += (predicted.cpu() == labels.cpu()).sum()\r\n                else:\r\n                    correct += (predicted == labels).sum()\r\n\r\n            accuracy = 100 * correct / total\r\n\r\n            # Print Loss\r\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))","metadata":{"tags":[],"cell_id":"fda3fed6-8d51-45cc-a45f-a31a3a853ae3"},"outputs":[{"name":"stdout","text":"Iteration: 500. Loss: 0.44918152689933777. Accuracy: 89\nIteration: 1000. Loss: 0.25105711817741394. Accuracy: 92\nIteration: 1500. Loss: 0.2771628797054291. Accuracy: 94\nIteration: 2000. Loss: 0.2538827657699585. Accuracy: 95\nIteration: 2500. Loss: 0.13324858248233795. Accuracy: 96\nIteration: 3000. Loss: 0.09865064918994904. Accuracy: 96\n","output_type":"stream"}],"execution_count":11}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"5c999cab-7776-4b6b-9265-880443a32d2a","deepnote_execution_queue":[]}}